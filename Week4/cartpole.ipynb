{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving CartPole with DQNs\n",
    "In this assignment you will make an RL agent capable of achieving 150+ average reward in the CartPole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\amit\\anaconda3\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\amit\\anaconda3\\lib\\site-packages (from gym) (1.24.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\amit\\anaconda3\\lib\\site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\amit\\anaconda3\\lib\\site-packages (from gym) (0.0.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make all necessary imports here\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from model import DQN, CustomDataset\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import imageio\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the CartPoleAgent class:\n",
    "- The constructor (\\_\\___init__\\_\\_) should initialize __gamma__ and __epsilon__ as class variables. It initializes online network, saves it and loads it again in target network (We do this so that both our target and online network are same during initialization)\n",
    "- The __choose_action()__ function should take the __Q(s, a)__ values vector for a state s as input, for example if __Q_s__ is the given input, __Q_s[0]__ represents __Q(s, 0)__, __Q_s[1]__ represents __Q(s, 1)__ and so on, and the function should output the chosen action (an integer) according to the current exploration strategy (For example choose random action with probability ε and choose action with highest Q(s, a) value with probability 1-ε)\n",
    "- The __train()__ function runs for a specific number of loops, in each loop:\n",
    "    - It generates training data using __generate_training_data()__ function and passes it to train_instance function of the online network (which trains the online network)\n",
    "    - It then saves the online network and loads that same saved function as target network\n",
    "    - Calls the __evaluate_performace()__ function\n",
    "    - Updates the value of epsilon as required\n",
    "- The __generate_training_data()__ function:\n",
    "    - Simulates lots of episodes/games/trajectories, it uses the online network for chossing actions, and the target netowrk for determining targets, it then stores all such states in an list/array/tensor and corresponding labels (i.e. targets) in another list/array/tensor.\n",
    "    - It then makes a __CustomDataset__ variable with these state and labels and returns it\n",
    "    - The CartPole environment terminates after 500 steps truncates itself after 500 steps in a single episode, you have to check this yourself and terminate the episode if it's length becomes >= 500\n",
    "    - The number of data and targets in the dataset returned should be large enough (around 5000-10000), so that when we choose any random datapoints, they satisy the iid condition\n",
    "- The __evaluate_performance()__ function calculates the average achieved reward with the current online network by simulating atleast 5 episodes (without any exploration as we are just calculating average reward), it then prints the average reward\n",
    "\n",
    "Generally you should see a rising trend in your average obtained reward\n",
    "\n",
    "Now some recommendations:\n",
    "- You need a good exploratory strategy, exponentially decaying exploration is prefered, you can start with ε=0.5 and then divide it by a constant after each training loop, so that it finally reaches a value of ε = 0.01\n",
    "- Whenever you use forward function of the DQN class in __generate_training_data()__ or __evaluate()__, make sure to detach the tensor so that it does not calculate gradients. You can detach any tensor \"__a__\" like:\n",
    "```\n",
    "    a = a.detach()\n",
    "```\n",
    "- 0.99 is a good value for Gamma\n",
    "\n",
    "Some more things you can do (Optional):\n",
    "- You can load an already saved PyTorch model with name \"model.pth\" into any variable network as follows:\n",
    "```\n",
    "    network = torch.load(\"model.pth)\n",
    "```\n",
    "- In the __evaluate()__ function, you can use __imageio__ library to make gifs of your agent playing the game (Google How!), but you have to initialize your environment as:\n",
    "```\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "```\n",
    "- In the __evaluate()__ function, you can calculate the Mean-Square Error of the model and store these values for each iterations and finally plot it to get an idea of how is your training going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CartPoleAgent:\n",
    "    def __init__(self, epsilon=0.5, gamma=0.99) -> None:\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.online_network = DQN()\n",
    "        torch.save(self.online_network.state_dict(), 'online_model.pth')\n",
    "        self.target_network = DQN()\n",
    "        self.target_network.load_state_dict(torch.load('online_model.pth'))\n",
    "        pass\n",
    "    def choose_action(self, Q_s) -> int:\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(len(Q_s))\n",
    "        else:\n",
    "            return np.argmax(Q_s)\n",
    "        pass\n",
    "\n",
    "    def generate_training_data(self) -> CustomDataset:\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        states = []\n",
    "        labels = []\n",
    "\n",
    "        for _ in range(5000):\n",
    "            state = env.reset()\n",
    "            episode_states = []\n",
    "            episode_labels = []\n",
    "\n",
    "            for _ in range(500):\n",
    "                if isinstance(state, dict):\n",
    "                    state_list = [np.array(state[key]) for key in state]\n",
    "                    state_array = np.concatenate(state_list)\n",
    "                    state_tensor = torch.from_numpy(state_array).float().view(1, -1)\n",
    "                elif isinstance(state, tuple):\n",
    "                    state_tensor = torch.cat([torch.from_numpy(s).float().view(1, -1) for s in state], dim=1)\n",
    "                else:\n",
    "                    state_tensor = torch.from_numpy(state).float().view(1, -1)\n",
    "\n",
    "                Q_values = self.online_network.forward(state_tensor).detach().numpy()\n",
    "                action = self.choose_action(Q_values)\n",
    "\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                target = reward + self.gamma * np.max(self.target_network.forward(next_state).detach().numpy())\n",
    "\n",
    "                episode_states.append(state)\n",
    "                episode_labels.append(target)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            states.extend(episode_states)\n",
    "            labels.extend(episode_labels)\n",
    "        env.close()\n",
    "\n",
    "        training_data = CustomDataset(np.array(states), np.array(labels))\n",
    "        return training_data\n",
    "        pass\n",
    "\n",
    "    def train_agent(self, num_loops=1000):\n",
    "        for loop in tqdm(range(num_loops)):\n",
    "            training_data = self.generate_training_data()\n",
    "            self.online_network.train_instance(training_data)\n",
    "            torch.save(self.online_network.state_dict(), 'online_model.pth')\n",
    "            self.target_network.load_state_dict(torch.load('online_model.pth'))\n",
    "            self.evaluate_performance(loop)\n",
    "            self.epsilon *= 0.99\n",
    "        pass\n",
    "\n",
    "    def evaluate_performance(self, iteration) -> None:\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        total_reward = 0\n",
    "        num_episodes = 5\n",
    "\n",
    "        for _ in range(num_episodes):\n",
    "            state = env.reset()\n",
    "\n",
    "            for _ in range(500):\n",
    "                q_values = self.online_network.forward(torch.from_numpy(state).float())\n",
    "                action = self.choose_action(q_values.detach().numpy())\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "        average_reward = total_reward / num_episodes\n",
    "        print(f'Iteration: {iteration}, Average Reward: {average_reward}')\n",
    "        env.close()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should run the below cell to start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got dict)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# This cell should not be changed\u001b[39;00m\n\u001b[0;32m      2\u001b[0m Agent \u001b[38;5;241m=\u001b[39m CartPoleAgent()\n\u001b[1;32m----> 3\u001b[0m Agent\u001b[38;5;241m.\u001b[39mtrain_agent()\n",
      "Cell \u001b[1;32mIn[2], line 62\u001b[0m, in \u001b[0;36mCartPoleAgent.train_agent\u001b[1;34m(self, num_loops)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_agent\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_loops\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m loop \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_loops)):\n\u001b[1;32m---> 62\u001b[0m         training_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_training_data()\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monline_network\u001b[38;5;241m.\u001b[39mtrain_instance(training_data)\n\u001b[0;32m     64\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monline_network\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monline_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m, in \u001b[0;36mCartPoleAgent.generate_training_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state_array)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m---> 33\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mfrom_numpy(s)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m state], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     31\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state_array)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m---> 33\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39mfrom_numpy(s)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m state], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected np.ndarray (got dict)"
     ]
    }
   ],
   "source": [
    "# This cell should not be changed\n",
    "Agent = CartPoleAgent()\n",
    "Agent.train_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
